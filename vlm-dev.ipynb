{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Image to a Sequence of Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbeddings(nn.Module):\n",
    "    def __init__(\n",
    "        self, img_size: int = 96, patch_size: int = 16, hidden_dim: int = 512\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        # Store the input image size, the patch size and hidden dimension\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Calculate the total number of patches\n",
    "        self.num_patches = (self.img_size // self.patch_size) ** 2\n",
    "\n",
    "        # Create a convolution to extract patch embeddings\n",
    "        # in_channels=3 asummes a 3-channel image (RGB)\n",
    "        # outp_channels=hidden_dim sets the number of output channels to match the hidden dimension\n",
    "        # kernel_size=patch_size and stride=patch_size ensuring each patch is embedded separately\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=3,\n",
    "            out_channels=self.hidden_dim,\n",
    "            kernel_size=self.patch_size,\n",
    "            stride=self.patch_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        # Extract patch embeddings from the input image\n",
    "        # Output shape: (batch_size, hidden_dim, (self.img_size // self.patch_size), (self.img_size // self.patch_size))\n",
    "        X = self.conv(X)\n",
    "\n",
    "        # Flatten the spatial dimensions (height and width) of the patch embeddings\n",
    "        # This step flattens the patch dimensions to a single dimension\n",
    "        # Output shape: (batch_size, hidden_dim, self.num_patches)\n",
    "        X = X.flatten(2)\n",
    "\n",
    "        # Transpose the dimensions to obtain the shape (batch_size, num_patches, hidden_dim)\n",
    "        # This step brings the num_patches dimension to the second position\n",
    "        # Output shape: (batch_size, self.num_patches, hidden_dim)\n",
    "        X = X.transpose(1, 2)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of image patches: torch.Size([1, 36, 512])\n"
     ]
    }
   ],
   "source": [
    "B, C, H, W = 1, 3, 96, 96  # Batch size, Channels, Height, Width\n",
    "X = torch.randn(B, C, H, W)\n",
    "\n",
    "patch_size = 16\n",
    "hidden_dim = 512\n",
    "\n",
    "patch_embeddings = PatchEmbeddings(\n",
    "    img_size=H, patch_size=patch_size, hidden_dim=hidden_dim\n",
    ")\n",
    "patches = patch_embeddings(X)\n",
    "print(f\"Shape of image patches: {patches.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "num_patches = (H // patch_size) ** 2\n",
    "assert patches.shape == (B, num_patches, hidden_dim), \"Output shape is incorrect\"\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Mechanism \n",
    "Attention Mechanism across both the vision encoder and language decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The implementation of the attention head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_embed: int,\n",
    "        head_size: int,\n",
    "        dropout: float = 0.1,\n",
    "        is_decoder: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Linear layer for Key projection\n",
    "        self.key = nn.Linear(in_features=n_embed, out_features=head_size, bias=False)\n",
    "\n",
    "        # Linear layer for Query projection\n",
    "        self.query = nn.Linear(in_features=n_embed, out_features=head_size, bias=False)\n",
    "\n",
    "        # Linear layer for Value projection\n",
    "        self.value = nn.Linear(in_features=n_embed, out_features=head_size, bias=False)\n",
    "\n",
    "        # Dropout layer for regularization to prevent overfitting\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Flag indicating wheter the head is used as a decoder\n",
    "        self.is_decoder = is_decoder\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Get batch size (B), sequence length (T), and embedding dimension (C) from the input tensor\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # Compute Key, Query, and Value projections\n",
    "        k = self.key(x)  # Shape: (B, T, head_size)\n",
    "        q = self.query(x)  # Shape: (B, T, head_size)\n",
    "        v = self.value(x)  # SHape: (B, T, head_size)\n",
    "\n",
    "        # Compute attention scores by taking the dot product of Query and Key\n",
    "        # and scaling by the square root of the embedding dimension\n",
    "        wei = q @ k.transpose(-2, -1) * (C**-0.5)  # Shape: (B, T, T)\n",
    "\n",
    "        if self.is_decoder:\n",
    "            # If this head is used in the decoder, apply causal mask to the attention scores\n",
    "            # to prevent attention to future positions\n",
    "            tril = torch.tril(torch.ones(T, T, dtype=torch.bool, device=x.device))\n",
    "            wei = wei.masked_fill(mask=tril == 0, value=float(\"-inf\"))\n",
    "\n",
    "        # Apply softmax to the attention scores to obtain attention probabilities\n",
    "        # Sum of probabilities for each row will be 1\n",
    "        wei = F.softmax(input=wei, dim=-1)  # Shape: (B, T, T)\n",
    "\n",
    "        # Apply Dropout to the attention probabilities for regularization\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        # Perform a weighted aggregation of values using the attention probabilities\n",
    "        out = wei @ v  # Shape: (B, T, head_size)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of output tensor: torch.Size([1, 36, 16])\n"
     ]
    }
   ],
   "source": [
    "B, T, C = patches.shape  # Batch size, Sequence length, Embedding dimension\n",
    "head_size = 16  # Size of the attention head\n",
    "\n",
    "head = Head(n_embed=C, head_size=head_size)\n",
    "output = head(patches)\n",
    "print(f\"Shape of output tensor: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "assert output.shape == (B, T, head_size), \"Output shape is incorrect\"\n",
    "print(\"Test passed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
