{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/studeni/Development/VLM-from-Scratch/.venv/lib/python3.10/site-packages/torch/_subclasses/functional_tensor.py:258: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Image to a Sequence of Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbeddings(nn.Module):\n",
    "    def __init__(\n",
    "        self, img_size: int = 96, patch_size: int = 16, hidden_dim: int = 512\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        # Store the input image size, the patch size and hidden dimension\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Calculate the total number of patches\n",
    "        self.num_patches = (self.img_size // self.patch_size) ** 2\n",
    "\n",
    "        # Create a convolution to extract patch embeddings\n",
    "        # in_channels=3 asummes a 3-channel image (RGB)\n",
    "        # outp_channels=hidden_dim sets the number of output channels to match the hidden dimension\n",
    "        # kernel_size=patch_size and stride=patch_size ensuring each patch is embedded separately\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=3,\n",
    "            out_channels=self.hidden_dim,\n",
    "            kernel_size=self.patch_size,\n",
    "            stride=self.patch_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        # Extract patch embeddings from the input image\n",
    "        # Output shape: (batch_size, hidden_dim, (self.img_size // self.patch_size), (self.img_size // self.patch_size))\n",
    "        X = self.conv(X)\n",
    "\n",
    "        # Flatten the spatial dimensions (height and width) of the patch embeddings\n",
    "        # This step flattens the patch dimensions to a single dimension\n",
    "        # Output shape: (batch_size, hidden_dim, self.num_patches)\n",
    "        X = X.flatten(2)\n",
    "\n",
    "        # Transpose the dimensions to obtain the shape (batch_size, num_patches, hidden_dim)\n",
    "        # This step brings the num_patches dimension to the second position\n",
    "        # Output shape: (batch_size, self.num_patches, hidden_dim)\n",
    "        X = X.transpose(1, 2)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of image patches: torch.Size([1, 36, 512])\n"
     ]
    }
   ],
   "source": [
    "B, C, H, W = 1, 3, 96, 96  # Batch size, Channels, Height, Width\n",
    "X = torch.randn(B, C, H, W)\n",
    "\n",
    "patch_size = 16\n",
    "hidden_dim = 512\n",
    "\n",
    "patch_embeddings = PatchEmbeddings(\n",
    "    img_size=H, patch_size=patch_size, hidden_dim=hidden_dim\n",
    ")\n",
    "patches = patch_embeddings(X)\n",
    "print(f\"Shape of image patches: {patches.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "num_patches = (H // patch_size) ** 2\n",
    "assert patches.shape == (B, num_patches, hidden_dim), \"Output shape is incorrect\"\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Mechanism \n",
    "Attention Mechanism across both the vision encoder and language decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The implementation of the Attention Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_embed: int,\n",
    "        head_size: int,\n",
    "        dropout: float = 0.1,\n",
    "        is_decoder: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Linear layer for Key projection\n",
    "        self.key = nn.Linear(in_features=n_embed, out_features=head_size, bias=False)\n",
    "\n",
    "        # Linear layer for Query projection\n",
    "        self.query = nn.Linear(in_features=n_embed, out_features=head_size, bias=False)\n",
    "\n",
    "        # Linear layer for Value projection\n",
    "        self.value = nn.Linear(in_features=n_embed, out_features=head_size, bias=False)\n",
    "\n",
    "        # Dropout layer for regularization to prevent overfitting\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Flag indicating wheter the head is used as a decoder\n",
    "        self.is_decoder = is_decoder\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Get batch size (B), sequence length (T), and embedding dimension (C) from the input tensor\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # Compute Key, Query, and Value projections\n",
    "        k = self.key(x)  # Shape: (B, T, head_size)\n",
    "        q = self.query(x)  # Shape: (B, T, head_size)\n",
    "        v = self.value(x)  # SHape: (B, T, head_size)\n",
    "\n",
    "        # Compute attention scores by taking the dot product of Query and Key\n",
    "        # and scaling by the square root of the embedding dimension\n",
    "        wei = q @ k.transpose(-2, -1) * (C**-0.5)  # Shape: (B, T, T)\n",
    "\n",
    "        if self.is_decoder:\n",
    "            # If this head is used in the decoder, apply causal mask to the attention scores\n",
    "            # to prevent attention to future positions\n",
    "            tril = torch.tril(torch.ones(T, T, dtype=torch.bool, device=x.device))\n",
    "            wei = wei.masked_fill(mask=tril == 0, value=float(\"-inf\"))\n",
    "\n",
    "        # Apply softmax to the attention scores to obtain attention probabilities\n",
    "        # Sum of probabilities for each row will be 1\n",
    "        wei = F.softmax(input=wei, dim=-1)  # Shape: (B, T, T)\n",
    "\n",
    "        # Apply Dropout to the attention probabilities for regularization\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        # Perform a weighted aggregation of values using the attention probabilities\n",
    "        out = wei @ v  # Shape: (B, T, head_size)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of output tensor: torch.Size([1, 36, 16])\n"
     ]
    }
   ],
   "source": [
    "B, T, C = patches.shape  # Batch size, Sequence length, Embedding dimension\n",
    "head_size = 16  # Size of the attention head\n",
    "\n",
    "head = Head(n_embed=C, head_size=head_size)\n",
    "output = head(patches)\n",
    "print(f\"Shape of output tensor: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "assert output.shape == (B, T, head_size), \"Output shape is incorrect\"\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The implementation of Multihead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_embed: int,\n",
    "        num_heads: int,\n",
    "        dropout: float = 0.1,\n",
    "        is_decoder: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Ensure that the embedding dimension is divisible by the number of heads\n",
    "        assert n_embed % num_heads == 0, \"n_embed must be divisible by num_heads!\"\n",
    "\n",
    "        # Create a ModuleList of attention heads\n",
    "        self.heads = nn.ModuleList(\n",
    "            modules=[\n",
    "                Head(\n",
    "                    n_embed=n_embed,\n",
    "                    head_size=n_embed // num_heads,\n",
    "                    dropout=dropout,\n",
    "                    is_decoder=is_decoder,\n",
    "                )\n",
    "                for _ in range(num_heads)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Linear layer for projecting the concatenated head outputs\n",
    "        self.proj = nn.Linear(in_features=n_embed, out_features=n_embed)\n",
    "\n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Apply each attention head to the input tensor\n",
    "        head_outputs = [\n",
    "            h(x) for h in self.heads\n",
    "        ]  # Shape: num_heads * (B, T, head_size)\n",
    "\n",
    "        # Concatenate the outputs from all heads along the last dimension\n",
    "        out = torch.cat(tensors=head_outputs, dim=-1)  # Shape: (B, T, m_embed)\n",
    "\n",
    "        # Apply the projection layer to the concatenated outputs\n",
    "        out = self.proj(out)  # Shape: (B, T, m_embed)\n",
    "\n",
    "        # Apply Dropout to the projected outputs for regularization\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 2\n",
    "dropout = 0.1\n",
    "mha = MultiHeadAttention(n_embed=C, num_heads=num_heads, dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of output tensor: torch.Size([1, 36, 512])\n"
     ]
    }
   ],
   "source": [
    "output = mha(patches)\n",
    "print(f\"Shape of output tensor: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "assert output.shape == (B, T, C), \"Output shape is incorrect\"\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self, n_embed: int, dropout: float = 0.1, is_decoder: bool = False\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Define the layers of the MLP\n",
    "        layers = [\n",
    "            # First linear layer that expands the input dimension from n_embed to 4 * n_embed\n",
    "            nn.Linear(in_features=n_embed, out_features=4 * n_embed),\n",
    "            # Activation function: ReLU if is_decoder is True, else GELU\n",
    "            nn.ReLU() if is_decoder else nn.GELU(),\n",
    "            # Second linear layer that projects the intermediate dimension back to n_embed\n",
    "            nn.Linear(in_features=4 * n_embed, out_features=n_embed),\n",
    "            # Dropout layer for regularization\n",
    "            nn.Dropout(p=dropout),\n",
    "        ]\n",
    "\n",
    "        # Create the MLP as a sequence of layers\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Pass the input through the MLP layers\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.1\n",
    "mlp = MLP(n_embed=C, dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of output tensor: torch.Size([1, 36, 512])\n"
     ]
    }
   ],
   "source": [
    "output = mlp(output)  # Previous output of the Multihead Attention\n",
    "print(f\"Shape of output tensor: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "assert output.shape == (B, T, C), \"Output shape is incorrect\"\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_embed: int,\n",
    "        num_heads: int,\n",
    "        dropout: float = 0.1,\n",
    "        is_decoder: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Layer normalization for the input to the attention layer\n",
    "        self.ln1 = nn.LayerNorm(normalized_shape=n_embed)\n",
    "\n",
    "        # Multi-head attention module\n",
    "        self.mhattn = MultiHeadAttention(\n",
    "            n_embed=n_embed, num_heads=num_heads, dropout=dropout, is_decoder=is_decoder\n",
    "        )\n",
    "\n",
    "        # Layer normalization for the input to the FFN\n",
    "        self.ln2 = nn.LayerNorm(normalized_shape=n_embed)\n",
    "\n",
    "        # Feed-forward neural network (FFN)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(in_features=n_embed, out_features=4 * n_embed),\n",
    "            nn.GELU(),  # Activation function\n",
    "            nn.Linear(\n",
    "                in_features=4 * n_embed, out_features=n_embed\n",
    "            ),  # Projection back to the original dimension\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Saving the input for residual connection\n",
    "        original_x = x\n",
    "\n",
    "        # Apply layer normalization to the input\n",
    "        x = self.ln1(x)\n",
    "\n",
    "        # Apply multi-head attention\n",
    "        mhattn_output = self.mhattn(x)\n",
    "\n",
    "        # Add the residual connection (original input) to the attention output\n",
    "        x = original_x + mhattn_output\n",
    "\n",
    "        # Apply later normalization to the input to the FFN\n",
    "        x = self.ln2(x)\n",
    "\n",
    "        # Apply the FFN\n",
    "        ffn_output = self.ffn(x)\n",
    "\n",
    "        # Apply the residual connection (input to the FFN) to the FFN output\n",
    "        x = x + ffn_output\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 2\n",
    "dropout = 0.1\n",
    "block = Block(n_embed=C, num_heads=num_heads, dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of output tensor: torch.Size([1, 36, 512])\n"
     ]
    }
   ],
   "source": [
    "output = block(patches)\n",
    "print(f\"Shape of output tensor: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "assert output.shape == (B, T, C), \"Output shape is incorrect\"\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision Encoder - Vision Transformer (ViT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining patchification logic and attention block in to ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size: int,\n",
    "        patch_size: int,\n",
    "        num_hiddens: int,\n",
    "        num_heads: int,\n",
    "        num_blocks: int,\n",
    "        emb_dropout: float,\n",
    "        block_dropout: float,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Patch embedding layer to convert the input image into patches\n",
    "        self.patch_embedding = PatchEmbeddings(\n",
    "            img_size=img_size, patch_size=patch_size, hidden_dim=num_hiddens\n",
    "        )\n",
    "\n",
    "        # Learnable classification token\n",
    "        self.cls_token = nn.Parameter(data=torch.zeros(size=(1, 1, num_hiddens)))\n",
    "\n",
    "        # Calculate the number of patches\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        # Learnable position embedding\n",
    "        self.pos_embedding = nn.Parameter(\n",
    "            data=torch.randn(size=(1, num_patches + 1, num_hiddens))\n",
    "        )\n",
    "\n",
    "        # Dropout layer for the embeddings\n",
    "        self.dropout = nn.Dropout(p=emb_dropout)\n",
    "\n",
    "        # Stack of transformer blocks\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block(\n",
    "                    n_embed=num_hiddens,\n",
    "                    num_heads=num_heads,\n",
    "                    dropout=block_dropout,\n",
    "                    is_decoder=False,\n",
    "                )\n",
    "                for _ in range(num_blocks)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Layer normalization for the final representation\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=num_hiddens)\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        # Convert the input image into patch embeddings\n",
    "        x = self.patch_embedding(X)  # Shape: (B, num_patches, num_hiddens)\n",
    "\n",
    "        # Expand the classification token to match the batch size\n",
    "        cls_tokens = self.cls_token.expand(\n",
    "            x.shape[0], -1, -1\n",
    "        )  # Shape: (B, 1, num_hiddens)\n",
    "\n",
    "        # Concatenate the classification token with the patch embeddings\n",
    "        x = torch.cat(\n",
    "            tensors=(cls_tokens, x), dim=1\n",
    "        )  # Shape: (B, num_patches + 1, num_hiddens)\n",
    "\n",
    "        # Add the position embedding to the patch embeddings\n",
    "        x += self.pos_embedding  # Shape: (B, num_patches + 1, num_hiddens)\n",
    "\n",
    "        # Apply dropout to the embeddings\n",
    "        x = self.dropout(x)  # Shape: (B, num_patches + 1, num_hiddens)\n",
    "\n",
    "        # Pass the embeddings through the transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)  # Shape: (B, num_patches + 1, num_hiddens)\n",
    "\n",
    "        # Apply layer normalization to the `[CLS]` token's final representation\n",
    "        x = self.layer_norm(x[:, 0])  # Shape: (B, num_hiddens)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, C, H, W = 2, 3, 96, 96  # Batch size, Channels, Height, Width\n",
    "X = torch.randn(B, C, H, W)\n",
    "vit = ViT(\n",
    "    img_size=H,\n",
    "    patch_size=16,\n",
    "    num_hiddens=64,\n",
    "    num_heads=2,\n",
    "    num_blocks=2,\n",
    "    emb_dropout=0.1,\n",
    "    block_dropout=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 64])\n"
     ]
    }
   ],
   "source": [
    "output = vit(X)\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "assert output.shape == (B, 64), \"Output shape is incorrect\"\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
