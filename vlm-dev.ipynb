{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Image to a Sequence of Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbeddings(nn.Module):\n",
    "    def __init__(\n",
    "        self, img_size: int = 96, patch_size: int = 16, hidden_dim: int = 512\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        # Store the input image size, the patch size and hidden dimension\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Calculate the total number of patches\n",
    "        self.num_patches = (self.img_size // self.patch_size) ** 2\n",
    "\n",
    "        # Create a convolution to extract patch embeddings\n",
    "        # in_channels=3 asummes a 3-channel image (RGB)\n",
    "        # outp_channels=hidden_dim sets the number of output channels to match the hidden dimension\n",
    "        # kernel_size=patch_size and stride=patch_size ensuring each patch is embedded separately\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=3,\n",
    "            out_channels=self.hidden_dim,\n",
    "            kernel_size=self.patch_size,\n",
    "            stride=self.patch_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        # Extract patch embeddings from the input image\n",
    "        # Output shape: (batch_size, hidden_dim, (self.img_size // self.patch_size), (self.img_size // self.patch_size))\n",
    "        X = self.conv(X)\n",
    "\n",
    "        # Flatten the spatial dimensions (height and width) of the patch embeddings\n",
    "        # This step flattens the patch dimensions to a single dimension\n",
    "        # Output shape: (batch_size, hidden_dim, self.num_patches)\n",
    "        X = X.flatten(2)\n",
    "\n",
    "        # Transpose the dimensions to obtain the shape (batch_size, num_patches, hidden_dim)\n",
    "        # This step brings the num_patches dimension to the second position\n",
    "        # Output shape: (batch_size, self.num_patches, hidden_dim)\n",
    "        X = X.transpose(1, 2)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of image patches: torch.Size([1, 36, 512])\n"
     ]
    }
   ],
   "source": [
    "B, C, H, W = 1, 3, 96, 96  # Batch size, Channels, Height, Width\n",
    "X = torch.randn(B, C, H, W)\n",
    "\n",
    "patch_size = 16\n",
    "hidden_dim = 512\n",
    "\n",
    "patch_embeddings = PatchEmbeddings(\n",
    "    img_size=H, patch_size=patch_size, hidden_dim=hidden_dim\n",
    ")\n",
    "patches = patch_embeddings(X)\n",
    "print(f\"Shape of image patches: {patches.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "num_patches = (H // patch_size) ** 2\n",
    "assert patches.shape == (B, num_patches, hidden_dim), \"Output shape is incorrect\"\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Mechanism \n",
    "Attention Mechanism across both the vision encoder and language decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The implementation of the Attention Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_embed: int,\n",
    "        head_size: int,\n",
    "        dropout: float = 0.1,\n",
    "        is_decoder: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Linear layer for Key projection\n",
    "        self.key = nn.Linear(in_features=n_embed, out_features=head_size, bias=False)\n",
    "\n",
    "        # Linear layer for Query projection\n",
    "        self.query = nn.Linear(in_features=n_embed, out_features=head_size, bias=False)\n",
    "\n",
    "        # Linear layer for Value projection\n",
    "        self.value = nn.Linear(in_features=n_embed, out_features=head_size, bias=False)\n",
    "\n",
    "        # Dropout layer for regularization to prevent overfitting\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Flag indicating wheter the head is used as a decoder\n",
    "        self.is_decoder = is_decoder\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Get batch size (B), sequence length (T), and embedding dimension (C) from the input tensor\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # Compute Key, Query, and Value projections\n",
    "        k = self.key(x)  # Shape: (B, T, head_size)\n",
    "        q = self.query(x)  # Shape: (B, T, head_size)\n",
    "        v = self.value(x)  # SHape: (B, T, head_size)\n",
    "\n",
    "        # Compute attention scores by taking the dot product of Query and Key\n",
    "        # and scaling by the square root of the embedding dimension\n",
    "        wei = q @ k.transpose(-2, -1) * (C**-0.5)  # Shape: (B, T, T)\n",
    "\n",
    "        if self.is_decoder:\n",
    "            # If this head is used in the decoder, apply causal mask to the attention scores\n",
    "            # to prevent attention to future positions\n",
    "            tril = torch.tril(torch.ones(T, T, dtype=torch.bool, device=x.device))\n",
    "            wei = wei.masked_fill(mask=tril == 0, value=float(\"-inf\"))\n",
    "\n",
    "        # Apply softmax to the attention scores to obtain attention probabilities\n",
    "        # Sum of probabilities for each row will be 1\n",
    "        wei = F.softmax(input=wei, dim=-1)  # Shape: (B, T, T)\n",
    "\n",
    "        # Apply Dropout to the attention probabilities for regularization\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        # Perform a weighted aggregation of values using the attention probabilities\n",
    "        out = wei @ v  # Shape: (B, T, head_size)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of output tensor: torch.Size([1, 36, 16])\n"
     ]
    }
   ],
   "source": [
    "B, T, C = patches.shape  # Batch size, Sequence length, Embedding dimension\n",
    "head_size = 16  # Size of the attention head\n",
    "\n",
    "head = Head(n_embed=C, head_size=head_size)\n",
    "output = head(patches)\n",
    "print(f\"Shape of output tensor: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "assert output.shape == (B, T, head_size), \"Output shape is incorrect\"\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The implementation of Multihead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_embed: int,\n",
    "        num_heads: int,\n",
    "        dropout: float = 0.1,\n",
    "        is_decoder: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Ensure that the embedding dimension is divisible by the number of heads\n",
    "        assert n_embed % num_heads == 0, \"n_embed must be divisible by num_heads!\"\n",
    "\n",
    "        # Create a ModuleList of attention heads\n",
    "        self.heads = nn.ModuleList(\n",
    "            modules=[\n",
    "                Head(\n",
    "                    n_embed=n_embed,\n",
    "                    head_size=n_embed // num_heads,\n",
    "                    dropout=dropout,\n",
    "                    is_decoder=is_decoder,\n",
    "                )\n",
    "                for _ in range(num_heads)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Linear layer for projecting the concatenated head outputs\n",
    "        self.proj = nn.Linear(in_features=n_embed, out_features=n_embed)\n",
    "\n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Apply each attention head to the input tensor\n",
    "        head_outputs = [\n",
    "            h(x) for h in self.heads\n",
    "        ]  # Shape: num_heads * (B, T, head_size)\n",
    "\n",
    "        # Concatenate the outputs from all heads along the last dimension\n",
    "        out = torch.cat(tensors=head_outputs, dim=-1)  # Shape: (B, T, m_embed)\n",
    "\n",
    "        # Apply the projection layer to the concatenated outputs\n",
    "        out = self.proj(out)  # Shape: (B, T, m_embed)\n",
    "\n",
    "        # Apply Dropout to the projected outputs for regularization\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 2\n",
    "dropout = 0.1\n",
    "mha = MultiHeadAttention(n_embed=C, num_heads=num_heads, dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of output tensor: torch.Size([1, 36, 512])\n"
     ]
    }
   ],
   "source": [
    "output = mha(patches)\n",
    "print(f\"Shape of output tensor: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "assert output.shape == (B, T, C), \"Output shape is incorrect\"\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self, n_embed: int, dropout: float = 0.1, is_decoder: bool = False\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Define the layers of the MLP\n",
    "        layers = [\n",
    "            # First linear layer that expands the input dimension from n_embed to 4 * n_embed\n",
    "            nn.Linear(in_features=n_embed, out_features=4 * n_embed),\n",
    "            # Activation function: ReLU if is_decoder is True, else GELU\n",
    "            nn.ReLU() if is_decoder else nn.GELU(),\n",
    "            # Second linear layer that projects the intermediate dimension back to n_embed\n",
    "            nn.Linear(in_features=4 * n_embed, out_features=n_embed),\n",
    "            # Dropout layer for regularization\n",
    "            nn.Dropout(p=dropout),\n",
    "        ]\n",
    "\n",
    "        # Create the MLP as a sequence of layers\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Pass the input through the MLP layers\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.1\n",
    "mlp = MLP(n_embed=C, dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of output tensor: torch.Size([1, 36, 512])\n"
     ]
    }
   ],
   "source": [
    "output = mlp(output)  # Previous output of the Multihead Attention\n",
    "print(f\"Shape of output tensor: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "assert output.shape == (B, T, C), \"Output shape is incorrect\"\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_embed: int,\n",
    "        num_heads: int,\n",
    "        dropout: float = 0.1,\n",
    "        is_decoder: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Layer normalization for the input to the attention layer\n",
    "        self.ln1 = nn.LayerNorm(normalized_shape=n_embed)\n",
    "\n",
    "        # Multi-head attention module\n",
    "        self.mhattn = MultiHeadAttention(\n",
    "            n_embed=n_embed, num_heads=num_heads, dropout=dropout, is_decoder=is_decoder\n",
    "        )\n",
    "\n",
    "        # Layer normalization for the input to the FFN\n",
    "        self.ln2 = nn.LayerNorm(normalized_shape=n_embed)\n",
    "\n",
    "        # Feed-forward neural network (FFN)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(in_features=n_embed, out_features=4 * n_embed),\n",
    "            nn.GELU(),  # Activation function\n",
    "            nn.Linear(\n",
    "                in_features=4 * n_embed, out_features=n_embed\n",
    "            ),  # Projection back to the original dimension\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Saving the input for residual connection\n",
    "        original_x = x\n",
    "\n",
    "        # Apply layer normalization to the input\n",
    "        x = self.ln1(x)\n",
    "\n",
    "        # Apply multi-head attention\n",
    "        mhattn_output = self.mhattn(x)\n",
    "\n",
    "        # Add the residual connection (original input) to the attention output\n",
    "        x = original_x + mhattn_output\n",
    "\n",
    "        # Apply later normalization to the input to the FFN\n",
    "        x = self.ln2(x)\n",
    "\n",
    "        # Apply the FFN\n",
    "        ffn_output = self.ffn(x)\n",
    "\n",
    "        # Apply the residual connection (input to the FFN) to the FFN output\n",
    "        x = x + ffn_output\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 2\n",
    "dropout = 0.1\n",
    "block = Block(n_embed=C, num_heads=num_heads, dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of output tensor: torch.Size([1, 36, 512])\n"
     ]
    }
   ],
   "source": [
    "output = block(patches)\n",
    "print(f\"Shape of output tensor: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "assert output.shape == (B, T, C), \"Output shape is incorrect\"\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision Encoder - Vision Transformer (ViT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining patchification logic and attention block in to ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size: int,\n",
    "        patch_size: int,\n",
    "        num_hiddens: int,\n",
    "        num_heads: int,\n",
    "        num_blocks: int,\n",
    "        emb_dropout: float,\n",
    "        block_dropout: float,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Patch embedding layer to convert the input image into patches\n",
    "        self.patch_embedding = PatchEmbeddings(\n",
    "            img_size=img_size, patch_size=patch_size, hidden_dim=num_hiddens\n",
    "        )\n",
    "\n",
    "        # Learnable classification token\n",
    "        self.cls_token = nn.Parameter(data=torch.zeros(size=(1, 1, num_hiddens)))\n",
    "\n",
    "        # Calculate the number of patches\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        # Learnable position embedding\n",
    "        self.pos_embedding = nn.Parameter(\n",
    "            data=torch.randn(size=(1, num_patches + 1, num_hiddens))\n",
    "        )\n",
    "\n",
    "        # Dropout layer for the embeddings\n",
    "        self.dropout = nn.Dropout(p=emb_dropout)\n",
    "\n",
    "        # Stack of transformer blocks\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block(\n",
    "                    n_embed=num_hiddens,\n",
    "                    num_heads=num_heads,\n",
    "                    dropout=block_dropout,\n",
    "                    is_decoder=False,\n",
    "                )\n",
    "                for _ in range(num_blocks)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Layer normalization for the final representation\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=num_hiddens)\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        # Convert the input image into patch embeddings\n",
    "        x = self.patch_embedding(X)  # Shape: (B, num_patches, num_hiddens)\n",
    "\n",
    "        # Expand the classification token to match the batch size\n",
    "        cls_tokens = self.cls_token.expand(\n",
    "            x.shape[0], -1, -1\n",
    "        )  # Shape: (B, 1, num_hiddens)\n",
    "\n",
    "        # Concatenate the classification token with the patch embeddings\n",
    "        x = torch.cat(\n",
    "            tensors=(cls_tokens, x), dim=1\n",
    "        )  # Shape: (B, num_patches + 1, num_hiddens)\n",
    "\n",
    "        # Add the position embedding to the patch embeddings\n",
    "        x += self.pos_embedding  # Shape: (B, num_patches + 1, num_hiddens)\n",
    "\n",
    "        # Apply dropout to the embeddings\n",
    "        x = self.dropout(x)  # Shape: (B, num_patches + 1, num_hiddens)\n",
    "\n",
    "        # Pass the embeddings through the transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)  # Shape: (B, num_patches + 1, num_hiddens)\n",
    "\n",
    "        # Apply layer normalization to the `[CLS]` token's final representation\n",
    "        x = self.layer_norm(x[:, 0])  # Shape: (B, num_hiddens)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, C, H, W = 2, 3, 96, 96  # Batch size, Channels, Height, Width\n",
    "X = torch.randn(B, C, H, W)\n",
    "vit = ViT(\n",
    "    img_size=H,\n",
    "    patch_size=16,\n",
    "    num_hiddens=64,\n",
    "    num_heads=2,\n",
    "    num_blocks=2,\n",
    "    emb_dropout=0.1,\n",
    "    block_dropout=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 64])\n"
     ]
    }
   ],
   "source": [
    "output = vit(X)\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "assert output.shape == (B, 64), \"Output shape is incorrect\"\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision-Language Projection Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunatelly, we can't directly concatenate ViT output to the text embeddings. <br>\n",
    "We need to project this from dimensionality of image embeddings from the vision transformer to the dimensionality of text embeddings.\n",
    "\n",
    "Why MLP for this part? If you want to train VLM with low resources you can do so by keeping both the pretrained vision encoder and language decoder frozen during the VLM training. Therefore, allocating more parameters to the connection module could enhance the overall VLM's ability to generalize and help in the downstream instruction-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalProjector(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_embed: int,\n",
    "        img_embed_dim: int,\n",
    "        dropout: float = 0.1,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Define the projection network\n",
    "        self.net = nn.Sequential(\n",
    "            # Linear layer to expand the image embedding dimension\n",
    "            nn.Linear(in_features=img_embed_dim, out_features=4 * img_embed_dim),\n",
    "            # GELU activation function\n",
    "            nn.GELU(),\n",
    "            # Linear layer to project the expanded image embeddings to the text embedding dimension\n",
    "            nn.Linear(in_features=4 * img_embed_dim, out_features=n_embed),\n",
    "            # Dropout layer for regularization\n",
    "            nn.Dropout(p=dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Pass the input through the projection network\n",
    "        x = self.net(x)  # Shape: (B, img_embed_dim) --> (B, n_embed)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, n_embed, img_embed_dim = 2, 64, 128\n",
    "X = torch.randn(size=(B, img_embed_dim))\n",
    "\n",
    "projector = MultiModalProjector(\n",
    "    n_embed=n_embed, img_embed_dim=img_embed_dim, dropout=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 64])\n"
     ]
    }
   ],
   "source": [
    "output = projector(X)\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "assert output.shape == (B, n_embed), \"Output shape is incorrect\"\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Decoder Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only thing that deviates from origianl implementation is that here projection module is integrated into decoder model class. <br>\n",
    "In contrary, when using pretrained models with HuggingFace (or any other library), you can directly feed embeddings as input to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLanguageModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_embed: int,\n",
    "        img_embed_dim: int,\n",
    "        vocab_size: int,\n",
    "        num_heads: int,\n",
    "        n_layer: int,\n",
    "        use_images: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.use_images = use_images\n",
    "\n",
    "        # Token embedding table\n",
    "        self.token_embedding_table = nn.Embedding(\n",
    "            num_embeddings=vocab_size, embedding_dim=n_embed\n",
    "        )\n",
    "\n",
    "        # Position embedding table\n",
    "        self.position_embedding_table = nn.Embedding(\n",
    "            num_embeddings=1000, embedding_dim=n_embed\n",
    "        )\n",
    "\n",
    "        if use_images:\n",
    "            # Image projection layer to align image embeddings with text embeddings\n",
    "            self.image_projection = MultiModalProjector(\n",
    "                n_embed=n_embed, img_embed_dim=img_embed_dim\n",
    "            )\n",
    "\n",
    "        # Stack of transformer decoder blocks\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[\n",
    "                Block(n_embed=n_embed, num_heads=num_heads, is_decoder=True)\n",
    "                for _ in range(n_layer)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Final layer normalization\n",
    "        self.ln_f = nn.LayerNorm(normalized_shape=n_embed)\n",
    "\n",
    "        # Language modeling head\n",
    "        self.lm_head = nn.Linear(in_features=n_embed, out_features=vocab_size)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        idx: torch.Tensor,\n",
    "        img_embeds: torch.Tensor = None,\n",
    "        targets: torch.Tensor = None,\n",
    "    ) -> torch.Tensor:\n",
    "        # Get token embeddings from the input indices\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "\n",
    "        if self.use_images:\n",
    "            # Project and concatenate image embeddings with token embeddings\n",
    "            img_emb = self.image_projection(img_embeds).unsqueeze(1)\n",
    "            tok_emb = torch.cat([img_emb, tok_emb], dim=1)\n",
    "\n",
    "        # Get position embeddings\n",
    "        pos_emb = self.position_embedding_table(\n",
    "            torch.arange(tok_emb.size(1), device=idx.device)\n",
    "        )\n",
    "\n",
    "        # Add position embeddings to token embeddings\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        # Pass through the transformer decoder blocks\n",
    "        x = self.blocks(x)\n",
    "\n",
    "        # Apply final layer normalization\n",
    "        x = self.ln_f(x)\n",
    "\n",
    "        # Get the logits from the language modeling head\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            if self.use_images and img_embeds is not None:\n",
    "                # Prepare targets by concatenating a dummy target for the image embedding\n",
    "                batch_size = idx.size(0)\n",
    "                targets = torch.cat(\n",
    "                    [\n",
    "                        torch.full(\n",
    "                            (batch_size, 1), -100, dtype=torch.long, device=idx.device\n",
    "                        ),\n",
    "                        targets,\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                )\n",
    "\n",
    "            # Compute the cross-entropy loss\n",
    "            loss = F.cross_entropy(\n",
    "                input=logits.view(-1, logits.size(-1)),\n",
    "                target=targets.view(-1),\n",
    "                ignore_index=-100,\n",
    "            )\n",
    "            return logits, loss\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def generate(\n",
    "        self, idx: torch.Tensor, img_embeds: torch.Tensor, max_new_tokens: int\n",
    "    ) -> torch.Tensor:\n",
    "        # Get the batch size and sequence length\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # Initialize the generated sequence with the input indices\n",
    "        generated = idx\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "\n",
    "        if self.use_images and img_embeds is not None:\n",
    "            # Project and concatenate image embeddings with token embeddings\n",
    "            img_emb = self.image_projection(img_embeds).unsqueeze(1)\n",
    "            current_output = torch.cat([img_emb, tok_emb], dim=1)\n",
    "        else:\n",
    "            current_output = tok_emb\n",
    "\n",
    "        # Generate new tokens iteratevely\n",
    "        for i in range(max_new_tokens):\n",
    "            # Get the current sequence length\n",
    "            T_current = current_output.shape[1]\n",
    "\n",
    "            # Get position embeddings for the current sequence length\n",
    "            current_pos_emb = self.position_embedding_table(\n",
    "                torch.arange(T_current, device=idx.device)\n",
    "            ).unsqueeze(0)\n",
    "\n",
    "            # Add position embeddings to the current output\n",
    "            current_output += current_pos_emb\n",
    "\n",
    "            # Pass through the transformer decoder blocks\n",
    "            for block in self.blocks:\n",
    "                current_output = block(current_output)\n",
    "\n",
    "            # Get the logits for the last token\n",
    "            logits = self.lm_head(current_output[:, -1, :])\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # Sample the next token based on the probability\n",
    "            idx_next = torch.multinomial(input=probs, num_samples=1)\n",
    "\n",
    "            # Concatenate the generated token to the generated sequence\n",
    "            generated = torch.cat([generated, idx_next], dim=1)\n",
    "\n",
    "            # Get the embeddings for the generated token\n",
    "            idx_next_emb = self.token_embedding_table(idx_next)\n",
    "\n",
    "            # Concatenate the generated token embeddings to the current output\n",
    "            current_output = torch.cat([current_output, idx_next_emb], dim=1)\n",
    "\n",
    "        return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([10, 51, 1000]), Loss: 7.145832061767578\n",
      "Generated sequence shape: torch.Size([10, 70])\n"
     ]
    }
   ],
   "source": [
    "n_embed, img_embed_dim, vocab_size, num_heads, n_layer = 128, 256, 1000, 8, 6\n",
    "# `n_layer` is used to represent number of decoder transformer blocks and num_blocks for the vision encoder to avoid confusion\n",
    "model = DecoderLanguageModel(\n",
    "    n_embed=n_embed,\n",
    "    img_embed_dim=img_embed_dim,\n",
    "    vocab_size=vocab_size,\n",
    "    num_heads=num_heads,\n",
    "    n_layer=n_layer,\n",
    "    use_images=True,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# Dummy input\n",
    "B, T = 10, 50\n",
    "idx = torch.randint(low=0, high=vocab_size, size=(B, T)).to(device)\n",
    "image_embeds = torch.randn(B, 256).to(device)  # Assume img_embed_dim is 256\n",
    "\n",
    "targets = torch.randint(0, vocab_size, (B, T)).to(\n",
    "    device\n",
    ")  # Only if you want to compute loss\n",
    "\n",
    "# Test forward pass\n",
    "# Check if you need to calculate loss by providing targets\n",
    "if targets is not None:\n",
    "    logits, loss = model(idx, image_embeds, targets)\n",
    "    print(f\"Logits shape: {logits.shape}, Loss: {loss}\")\n",
    "else:\n",
    "    logits = model(idx, image_embeds)  # Call without targets\n",
    "    print(f\"Logits shape: {logits.shape}\")\n",
    "\n",
    "# Test generation\n",
    "generated = model.generate(idx, image_embeds, max_new_tokens=20)\n",
    "print(f\"Generated sequence shape: {generated.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting everything together: Simple Vision Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionLanguageModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_embed: int,\n",
    "        img_embed_dim: int,\n",
    "        vocab_size: int,\n",
    "        n_layer: int,\n",
    "        img_size: int,\n",
    "        patch_size: int,\n",
    "        num_heads: int,\n",
    "        num_blocks: int,\n",
    "        emb_dropout: float,\n",
    "        block_dropout: float,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Set num_hiddens equal to img_embed_dim\n",
    "        num_hiddens = img_embed_dim\n",
    "\n",
    "        # Assert that num_hiddens is divisible by num_heads\n",
    "        assert num_hiddens % num_heads == 0, ValueError(\n",
    "            \"num_hiddens must be divisible by num_heads!\"\n",
    "        )\n",
    "\n",
    "        # Initialize the Vision Transformer (ViT) encoder\n",
    "        self.vision_encoder = ViT(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            num_hiddens=num_hiddens,\n",
    "            num_heads=num_heads,\n",
    "            num_blocks=num_blocks,\n",
    "            emb_dropout=emb_dropout,\n",
    "            block_dropout=block_dropout,\n",
    "        )\n",
    "\n",
    "        # Initialize the Language Model Decoder (DecoderLanguageModel)\n",
    "        self.decoder = DecoderLanguageModel(\n",
    "            n_embed=n_embed,\n",
    "            img_embed_dim=img_embed_dim,\n",
    "            vocab_size=vocab_size,\n",
    "            num_heads=num_heads,\n",
    "            n_layer=n_layer,\n",
    "            use_images=True,\n",
    "        )\n",
    "\n",
    "    def _check_image_embeddings(self, image_embeds: torch.Tensor) -> None:\n",
    "        \"\"\"Chek if image embeddings are valid.\"\"\"\n",
    "        if image_embeds.nelement() == 0 or image_embeds.shape[1] == 0:\n",
    "            raise ValueError(\n",
    "                \"Something is wrong with the ViT model. It's returning an empty tensor or the embedding dimension is empty.\"\n",
    "            )\n",
    "\n",
    "    def forward(\n",
    "        self, img_array: torch.Tensor, idx: torch.Tensor, targets: torch.Tensor = None\n",
    "    ) -> torch.Tensor | Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Get the image embeddings from the Vision Encoder\n",
    "        image_embeds = self.vision_encoder(img_array)\n",
    "\n",
    "        # Check if image embeddings are valid\n",
    "        self._check_image_embeddings(image_embeds)\n",
    "\n",
    "        if targets is not None:\n",
    "            # If targets are provided, compute the logits and loss\n",
    "            logits, loss = self.decoder(idx, image_embeds, targets)\n",
    "            return logits, loss\n",
    "        else:\n",
    "            # If targets are not provided, compute only the logits\n",
    "            logits = self.decoder(idx, image_embeds)\n",
    "            return logits\n",
    "\n",
    "    def generate(\n",
    "        self, img_array: torch.Tensor, idx: torch.Tensor, max_new_tokens: int\n",
    "    ) -> torch.Tensor:\n",
    "        # Get the image embeddings from the Vision Encoder\n",
    "        image_embeds = self.vision_encoder(img_array)\n",
    "\n",
    "        # Check if image embeddings are valid\n",
    "        self._check_image_embeddings(image_embeds)\n",
    "\n",
    "        # Generate new tokens using the Language Model Decoder\n",
    "        generated_tokens = self.decoder.generate(\n",
    "            idx=idx, img_embeds=image_embeds, max_new_tokens=max_new_tokens\n",
    "        )\n",
    "        return generated_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Running on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from initialization forward pass: tensor([[[-0.8860, -0.0301,  0.1804,  ...,  0.4420,  0.0458, -1.0360],\n",
      "         [-0.3543,  0.1468,  0.0166,  ...,  0.8835,  0.1615, -0.4509],\n",
      "         [-0.3524,  0.2126,  1.6317,  ...,  1.0799,  0.1788, -0.4753],\n",
      "         ...,\n",
      "         [ 0.3872,  0.4607,  0.5435,  ..., -0.0981, -0.8271, -0.9599],\n",
      "         [-0.3231, -0.1148, -0.0585,  ...,  1.1575, -0.6290, -0.6378],\n",
      "         [ 0.0722, -0.1574, -0.3214,  ...,  0.2728, -0.7626,  0.2860]]],\n",
      "       device='mps:0', grad_fn=<LinearBackward0>)\n"
     ]
    }
   ],
   "source": [
    "n_embed, num_hiddens, vocab_size, num_heads, n_layer = 128, 512, 1000, 8, 8\n",
    "image_embed_dim = num_hiddens\n",
    "img_size = 96\n",
    "patch_size = 16\n",
    "num_blocks = 2\n",
    "\n",
    "n_layer, block_size, num_hiddens = 8, 32, 512\n",
    "\n",
    "# Initialize the model\n",
    "model = VisionLanguageModel(\n",
    "    n_embed=n_embed,\n",
    "    img_embed_dim=image_embed_dim,\n",
    "    vocab_size=vocab_size,\n",
    "    n_layer=n_layer,\n",
    "    img_size=img_size,\n",
    "    patch_size=patch_size,\n",
    "    num_heads=num_heads,\n",
    "    num_blocks=num_blocks,\n",
    "    emb_dropout=0.1,\n",
    "    block_dropout=0.1,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# Create dummy data with correct dimensions\n",
    "dummy_img = torch.randn(1, 3, img_size, img_size).to(\n",
    "    device\n",
    ")  # Correct shape for image input\n",
    "dummy_idx = torch.randint(0, vocab_size, (1, block_size)).to(\n",
    "    device\n",
    ")  # Correct shape for text input\n",
    "\n",
    "# Forward pass to initialize all parameters\n",
    "try:\n",
    "    output = model(dummy_img, dummy_idx)  # Output for debugging\n",
    "    print(\"Output from initialization forward pass:\", output)\n",
    "except RuntimeError as e:\n",
    "    print(f\"Runtime Error during forward pass: {str(e)}\")\n",
    "    print(\"Check layer configurations and input shapes.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
